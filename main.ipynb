{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586914f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 算法 keras 自定义loss  Matlab data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "import scipy.io as scio\n",
    "\n",
    "# DeepNN topology\n",
    "layers=[2,20,20,20,20,20,20,3]\n",
    "# layers=[2,50,50,20,1]\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "   learning_rate=0.002, beta_1=0.99, epsilon=1e-1)\n",
    "# tf_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "xmax=0\n",
    "xmin=-1.5\n",
    "ymax=0.8\n",
    "ymin=-0.8\n",
    "\n",
    "# sampling points\n",
    "NH=100000\n",
    "x1=tf.random.uniform([NH],xmin,xmax,dtype=tf.float32)\n",
    "x2=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "\n",
    "class mymodel():\n",
    "    def __init__(self,layers,optimizer,x1,x2):\n",
    "        self.u_model=tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(lambda X: X))\n",
    "        for width in layers[1:7]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "            width, activation=tf.nn.tanh,\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        self.u_model.add(tf.keras.layers.Dense(\n",
    "            layers[7],\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "        \n",
    "        self.optimizer=optimizer\n",
    "        self.dtype=tf.float32\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self):\n",
    "        Xnode=tf.constant([-1,0],shape=(1,2),dtype=tf.float32)\n",
    "        u_pred=self.u_model(Xnode)\n",
    "        u_pred0=u_pred[:,0]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x1)\n",
    "            tape.watch(x2)\n",
    "            X_f = tf.stack([x1,x2], axis=1)\n",
    "            u = self.u_model(X_f)\n",
    "            u0=u[:,0]\n",
    "            u1=u[:,1]\n",
    "            u2=u[:,2]\n",
    "    \n",
    "        u_x1=tape.gradient(u0, x1)\n",
    "        #u_x1=tf.reduce_mean(u_x1,axis=1)\n",
    "        u_x1=u_x1+2*x1+2\n",
    "        u_x2=tape.gradient(u0, x2)\n",
    "        #u_x2=tf.reduce_mean(u_x2,axis=1)\n",
    "        u_x2=u_x2+2*x2\n",
    "        \n",
    "        alpha=0.5\n",
    "        beta=3\n",
    "        b1=-x1**3+x1-alpha*beta*x1*x2\n",
    "        b2=beta*(x1**4-x1**2)-alpha*x2\n",
    "        delta=0.001\n",
    "        \n",
    "        return tf.reduce_mean(tf.square(b1+0.5*u_x1-u1),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(b2+0.5*u_x2-u2),axis=0) * 1 + \\\n",
    "            tf.reduce_mean((u_x1*u1+u_x2*u2)**2/((u_x1**2+u_x2**2)*(u1**2+u2**2)+delta),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(u_pred0),axis=0) * 0.1\n",
    "\n",
    "    def __grad(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss()\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, tf_epochs=5000):\n",
    "    # Creating the tensors\n",
    "    #X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    #u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "    #print(self.__wrap_training_variables())\n",
    "\n",
    "        LOSS=np.zeros([1,tf_epochs])\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad()\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            LOSS[0,epoch]=loss_value.numpy()\n",
    "            print('epoch, loss_value:', epoch, loss_value)\n",
    "\n",
    "        scio.savemat('LOSS.mat',{'LOSS':LOSS})\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        # f_star = self.H_model()\n",
    "        return u_star#, f_star\n",
    "\n",
    "\n",
    "model=mymodel(layers, tf_optimizer, x1, x2)\n",
    "\n",
    "# checkpoint_save_path=\"./checkpoint/mnist.ckpt\"\n",
    "# if os.path.exists(checkpoint_save_path + '.index'):\n",
    "#   print('-------------------load the model---------------------')\n",
    "#   model.load_weights(checkpoint_save_path)\n",
    "\n",
    "# cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "#                         save_weights_only=True,\n",
    "#                         save_best_only=True)\n",
    "\n",
    "history=model.fit(tf_epochs=100000)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "x1=tf.constant([-1.5,-1.5,-1.5,-1,-1,-1,0,0,0],dtype=tf.float32)\n",
    "y1=tf.constant([-0.8,0,0.8,-0.8,0,0.8,-0.8,0,0.8],dtype=tf.float32)\n",
    "X_star = tf.stack([x1,y1], axis=1)\n",
    "print('X_star:\\n ', X_star)\n",
    "u_pred= model.predict(X_star)\n",
    "print('u_pred:\\n ', u_pred)\n",
    "\n",
    "dataFile = \"xtest\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "xtest = data['xtest']\n",
    "dataFile = \"ytest\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "ytest = data['ytest']\n",
    "xtest =tf.reduce_sum(xtest, axis=0)\n",
    "ytest =tf.reduce_sum(ytest, axis=0)\n",
    "X_star = tf.stack([xtest,ytest], axis=1)\n",
    "Stest= model.predict(X_star)\n",
    "scio.savemat('Stest.mat',{'Stest':Stest.numpy()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing MPEP and prefactor\n",
    "alpha=0.5\n",
    "beta=3\n",
    "H_bar=np.zeros([2,2])\n",
    "H_bar[0,0]=4\n",
    "H_bar[1,1]=2*alpha\n",
    "detH_bar=8*alpha\n",
    "H_star=np.zeros([2,2])\n",
    "H_star[0,0]=-2\n",
    "H_star[1,1]=2*alpha\n",
    "detH_star=4*alpha\n",
    "lambda_star=1\n",
    "pi=3.141592653\n",
    "\n",
    "h=0.001\n",
    "nT=10000\n",
    "x1=np.zeros(nT,)\n",
    "x2=np.zeros(nT,)\n",
    "divL=np.zeros(nT,)\n",
    "#normbx=np.zeros(nT,)\n",
    "x1[0]=-0.05*0.5**0.5\n",
    "x2[0]=0.05*0.5**0.5\n",
    "\n",
    "for epoch in range(nT-1):\n",
    "    x01=tf.constant(x1[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(x2[epoch],shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "        u1=u[:,1]\n",
    "        u2=u[:,2]\n",
    "    # Getting the derivative\n",
    "    u_x = tape.gradient(u0, x01)\n",
    "    u_y = tape.gradient(u0, x02)\n",
    "    u_x = u_x+2*x01+2\n",
    "    u_y = u_y+2*x02\n",
    "    lx_x = tape.gradient(u1, x01)\n",
    "    ly_y = tape.gradient(u2, x02)\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    divL[epoch]=lx_x.numpy()+ly_y.numpy()\n",
    "    \n",
    "    if ((x1[epoch]+1.0)**2+(x2[epoch])**2)<0.01:\n",
    "        break\n",
    "    \n",
    "    p1=u_x.numpy()\n",
    "    p2=u_y.numpy()\n",
    "    bx1=-x1[epoch]**3 + x1[epoch] - alpha*beta*x1[epoch]*x2[epoch]\n",
    "    bx2=beta*(x1[epoch]**4 - x1[epoch]**2) - alpha*x2[epoch]\n",
    "    normb=(bx1**2+bx2**2)**0.5\n",
    "    # normbx[epoch]=normb\n",
    "    x1[epoch+1]=x1[epoch] -h * (bx1 + p1)/normb\n",
    "    x2[epoch+1]=x2[epoch] -h * (bx2 + p2)/normb\n",
    "\n",
    "print('epoch:',epoch)\n",
    "MPEP=tf.stack([x1[0:epoch+1],x2[0:epoch+1]], axis=1)\n",
    "scio.savemat('MPEP.mat',{'MPEP':MPEP.numpy()})\n",
    "print(MPEP)\n",
    "integ=np.sum(divL[0:epoch+1])*h\n",
    "prefactor=pi/lambda_star**(detH_star/detH_bar)**0.5*np.exp(integ)\n",
    "print('integ:',integ)\n",
    "print('prefactor:',prefactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28456f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
