{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586914f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 算法 keras 自定义loss  Matlab data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "import scipy.io as scio\n",
    "\n",
    "# DeepNN topology\n",
    "layers=[5,20,20,20,20,20,20,6]\n",
    "# layers=[2,50,50,20,1]\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "   learning_rate=0.002, beta_1=0.99, epsilon=1e-1)\n",
    "# tf_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "xmax=0\n",
    "xmin=-1.5\n",
    "ymax=1.0\n",
    "ymin=-1.0\n",
    "\n",
    "# sampling points\n",
    "NH=100000\n",
    "x1=tf.random.uniform([NH],xmin,xmax,dtype=tf.float32)\n",
    "x2=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "x3=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "x4=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "x5=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "\n",
    "class mymodel():\n",
    "    def __init__(self,layers,optimizer,x1,x2,x3,x4,x5):\n",
    "        self.u_model=tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(lambda X: X))\n",
    "        for width in layers[1:7]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "            width, activation=tf.nn.tanh,\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        self.u_model.add(tf.keras.layers.Dense(\n",
    "            layers[7],\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "        \n",
    "        self.optimizer=optimizer\n",
    "        self.dtype=tf.float32\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self):\n",
    "        Xnode=tf.constant([-1,0,0,0,0],shape=(1,5),dtype=tf.float32)\n",
    "        u_pred=self.u_model(Xnode)\n",
    "        u_pred0=u_pred[:,0]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x1)\n",
    "            tape.watch(x2)\n",
    "            tape.watch(x3)\n",
    "            tape.watch(x4)\n",
    "            tape.watch(x5)\n",
    "            X_f = tf.stack([x1,x2,x3,x4,x5], axis=1)\n",
    "            u = self.u_model(X_f)\n",
    "            u0=u[:,0]\n",
    "            u1=u[:,1]\n",
    "            u2=u[:,2]\n",
    "            u3=u[:,3]\n",
    "            u4=u[:,4]\n",
    "            u5=u[:,5]\n",
    "    \n",
    "        u_x1=tape.gradient(u0, x1)\n",
    "        #u_x1=tf.reduce_mean(u_x1,axis=1)\n",
    "        u_x1=u_x1+2*x1+2\n",
    "        u_x2=tape.gradient(u0, x2)\n",
    "        u_x2=u_x2+2*x2\n",
    "        u_x3=tape.gradient(u0, x3)\n",
    "        u_x3=u_x3+2*x3\n",
    "        u_x4=tape.gradient(u0, x4)\n",
    "        u_x4=u_x4+2*x4\n",
    "        u_x5=tape.gradient(u0, x5)\n",
    "        u_x5=u_x5+2*x5\n",
    "        \n",
    "        b1=-2*(x1**3-x1)-(x2+x3+x4+x5)\n",
    "        b2=-x2+2*(x1**3-x1)\n",
    "        b3=-x3+2*(x1**3-x1)\n",
    "        b4=-x4+2*(x1**3-x1)\n",
    "        b5=-x5+2*(x1**3-x1)\n",
    "        delta=0.001\n",
    "        \n",
    "        return tf.reduce_mean(tf.square(b1+0.5*u_x1-u1),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(b2+0.5*u_x2-u2),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(b3+0.5*u_x3-u3),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(b4+0.5*u_x4-u4),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(b5+0.5*u_x5-u5),axis=0) * 1 + \\\n",
    "            tf.reduce_mean((u_x1*u1+u_x2*u2+u_x3*u3+u_x4*u4+u_x5*u5)**2/((u_x1**2+u_x2**2+u_x3**2+u_x4**2+u_x5**2)*(u1**2+u2**2+u3**2+u4**2+u5**2)+delta),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(u_pred0),axis=0) * 0.1\n",
    "\n",
    "    def __grad(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss()\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, tf_epochs=5000):\n",
    "    # Creating the tensors\n",
    "    #X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    #u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "    #print(self.__wrap_training_variables())\n",
    "\n",
    "        LOSS=np.zeros([1,tf_epochs])\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad()\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            LOSS[0,epoch]=loss_value.numpy()\n",
    "            print('epoch, loss_value:', epoch, loss_value)\n",
    "\n",
    "        scio.savemat('LOSS.mat',{'LOSS':LOSS})\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        # f_star = self.H_model()\n",
    "        return u_star#, f_star\n",
    "\n",
    "\n",
    "model=mymodel(layers, tf_optimizer, x1, x2, x3, x4, x5)\n",
    "\n",
    "# checkpoint_save_path=\"./checkpoint/mnist.ckpt\"\n",
    "# if os.path.exists(checkpoint_save_path + '.index'):\n",
    "#   print('-------------------load the model---------------------')\n",
    "#   model.load_weights(checkpoint_save_path)\n",
    "\n",
    "# cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "#                         save_weights_only=True,\n",
    "#                         save_best_only=True)\n",
    "\n",
    "history=model.fit(tf_epochs=100000)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "x01=tf.constant([-1.5,-1.5,-1.5,-1,-1,-1,0,0,0],dtype=tf.float32)\n",
    "x02=tf.constant([-0.8,0,0.8,-0.8,0,0.8,-0.8,0,0.8],dtype=tf.float32)\n",
    "x03=tf.constant([-0.8,0,0.8,-0.8,0,0.8,-0.8,0,0.8],dtype=tf.float32)\n",
    "x04=tf.constant([-0.8,0,0.8,-0.8,0,0.8,-0.8,0,0.8],dtype=tf.float32)\n",
    "x05=tf.constant([-0.8,0,0.8,-0.8,0,0.8,-0.8,0,0.8],dtype=tf.float32)\n",
    "X_star = tf.stack([x01,x02,x03,x04,x05], axis=1)\n",
    "print('X_star:\\n ', X_star)\n",
    "u_pred= model.predict(X_star)\n",
    "print('u_pred:\\n ', u_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing Stest\n",
    "dataFile = \"x1test\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x1test = data['x1test']\n",
    "dataFile = \"x2test\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x2test = data['x2test']\n",
    "dataFile = \"x3test\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x3test = data['x3test']\n",
    "dataFile = \"x4test\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x4test = data['x4test']\n",
    "dataFile = \"x5test\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x5test = data['x5test']\n",
    "#print(x1test)\n",
    "\n",
    "x1test =tf.reduce_sum(x1test, axis=0)\n",
    "x2test =tf.reduce_sum(x2test, axis=0)\n",
    "x3test =tf.reduce_sum(x3test, axis=0)\n",
    "x4test =tf.reduce_sum(x4test, axis=0)\n",
    "x5test =tf.reduce_sum(x5test, axis=0)\n",
    "#print(x1test)\n",
    "X_star = tf.stack([x1test,x2test,x3test,x4test,x5test], axis=1)\n",
    "Stest= model.predict(X_star)\n",
    "scio.savemat('Stest.mat',{'Stest':Stest.numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fa0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding the location of minimal quasi-potential on the boundary x1=-0.5\n",
    "xm1=-0.5\n",
    "xm2=0.5\n",
    "xm3=0\n",
    "xm4=0\n",
    "xm5=0.5\n",
    "eta=0.01\n",
    "\n",
    "for epoch in range(1000):\n",
    "    x01=tf.constant(xm1,shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(xm2,shape=(1,),dtype=tf.float32)\n",
    "    x03=tf.constant(xm3,shape=(1,),dtype=tf.float32)\n",
    "    x04=tf.constant(xm4,shape=(1,),dtype=tf.float32)\n",
    "    x05=tf.constant(xm5,shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        tape.watch(x03)\n",
    "        tape.watch(x04)\n",
    "        tape.watch(x05)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02,x03,x04,x05], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "    # Getting the derivative\n",
    "    u_x2 = tape.gradient(u0, x02)\n",
    "    u_x3 = tape.gradient(u0, x03)\n",
    "    u_x4 = tape.gradient(u0, x04)\n",
    "    u_x5 = tape.gradient(u0, x05)\n",
    "    u_x2 = u_x2+2*x02\n",
    "    u_x3 = u_x3+2*x03\n",
    "    u_x4 = u_x4+2*x04\n",
    "    u_x5 = u_x5+2*x05\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    \n",
    "    p2=u_x2.numpy()\n",
    "    p3=u_x3.numpy()\n",
    "    p4=u_x4.numpy()\n",
    "    p5=u_x5.numpy()\n",
    "    \n",
    "    xm2=xm2-eta*p2\n",
    "    xm3=xm3-eta*p3\n",
    "    xm4=xm4-eta*p4\n",
    "    xm5=xm5-eta*p5\n",
    "    print(epoch,xm1,xm2,xm3,xm4,xm5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing MPEP and prefactor characteristic boundary\n",
    "detH_bar=128.0\n",
    "detH_star=64.0\n",
    "lambda_star=1\n",
    "pi=3.141592653\n",
    "\n",
    "h=0.001\n",
    "nT=10000\n",
    "x1=np.zeros(nT,)\n",
    "x2=np.zeros(nT,)\n",
    "x3=np.zeros(nT,)\n",
    "x4=np.zeros(nT,)\n",
    "x5=np.zeros(nT,)\n",
    "divL=np.zeros(nT,)\n",
    "#normbx=np.zeros(nT,)\n",
    "x1[0]=-0.05\n",
    "x2[0]=0\n",
    "x3[0]=0\n",
    "x4[0]=0\n",
    "x5[0]=0\n",
    "\n",
    "for epoch in range(nT-1):\n",
    "    x01=tf.constant(x1[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(x2[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x03=tf.constant(x3[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x04=tf.constant(x4[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x05=tf.constant(x5[epoch],shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        tape.watch(x03)\n",
    "        tape.watch(x04)\n",
    "        tape.watch(x05)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02,x03,x04,x05], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "        u1=u[:,1]\n",
    "        u2=u[:,2]\n",
    "        u3=u[:,3]\n",
    "        u4=u[:,4]\n",
    "        u5=u[:,5]\n",
    "    # Getting the derivative\n",
    "    u_x1 = tape.gradient(u0, x01)\n",
    "    u_x2 = tape.gradient(u0, x02)\n",
    "    u_x3 = tape.gradient(u0, x03)\n",
    "    u_x4 = tape.gradient(u0, x04)\n",
    "    u_x5 = tape.gradient(u0, x05)\n",
    "    u_x1 = u_x1+2*x01+2\n",
    "    u_x2 = u_x2+2*x02\n",
    "    u_x3 = u_x3+2*x03\n",
    "    u_x4 = u_x4+2*x04\n",
    "    u_x5 = u_x5+2*x05\n",
    "    l1_x1 = tape.gradient(u1, x01)\n",
    "    l2_x2 = tape.gradient(u2, x02)\n",
    "    l3_x3 = tape.gradient(u3, x03)\n",
    "    l4_x4 = tape.gradient(u4, x04)\n",
    "    l5_x5 = tape.gradient(u5, x05)\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    divL[epoch]=l1_x1.numpy()+l2_x2.numpy()+l3_x3.numpy()+l4_x4.numpy()+l5_x5.numpy()\n",
    "    \n",
    "    if ((x1[epoch]+1.0)**2+(x2[epoch])**2+(x3[epoch])**2+(x4[epoch])**2+(x5[epoch])**2)**0.5<0.01:\n",
    "        break\n",
    "    \n",
    "    p1=u_x1.numpy()\n",
    "    p2=u_x2.numpy()\n",
    "    p3=u_x3.numpy()\n",
    "    p4=u_x4.numpy()\n",
    "    p5=u_x5.numpy()\n",
    "    bx1=-2*(x1[epoch]**3-x1[epoch])-(x2[epoch]+x3[epoch]+x4[epoch]+x5[epoch])\n",
    "    bx2=-x2[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    bx3=-x3[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    bx4=-x4[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    bx5=-x5[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    normb=(bx1**2+bx2**2+bx3**2+bx4**2+bx5**2)**0.5\n",
    "    # normbx[epoch]=normb\n",
    "    x1[epoch+1]=x1[epoch] -h * (bx1 + p1)/normb\n",
    "    x2[epoch+1]=x2[epoch] -h * (bx2 + p2)/normb\n",
    "    x3[epoch+1]=x3[epoch] -h * (bx3 + p3)/normb\n",
    "    x4[epoch+1]=x4[epoch] -h * (bx4 + p4)/normb\n",
    "    x5[epoch+1]=x5[epoch] -h * (bx5 + p5)/normb\n",
    "\n",
    "print('epoch:',epoch)\n",
    "MPEP=tf.stack([x1[0:epoch+1],x2[0:epoch+1],x3[0:epoch+1],x4[0:epoch+1],x5[0:epoch+1]], axis=1)\n",
    "scio.savemat('MPEP.mat',{'MPEP':MPEP.numpy()})\n",
    "print(MPEP)\n",
    "integ=np.sum(divL[0:epoch+1])*h\n",
    "prefactor=pi/lambda_star*(detH_star/detH_bar)**0.5*np.exp(integ)\n",
    "print('integ:',integ)\n",
    "print('prefactor:',prefactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30675e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing MPEP and prefactor non-characteristic boundary\n",
    "detH_bar=128.0\n",
    "deth2_star=16.0\n",
    "miu2_star=0.75\n",
    "pi=3.141592653\n",
    "\n",
    "h=0.001\n",
    "nT=10000\n",
    "x1=np.zeros(nT,)\n",
    "x2=np.zeros(nT,)\n",
    "x3=np.zeros(nT,)\n",
    "x4=np.zeros(nT,)\n",
    "x5=np.zeros(nT,)\n",
    "divL=np.zeros(nT,)\n",
    "#normbx=np.zeros(nT,)\n",
    "x1[0]=-0.5\n",
    "x2[0]=0.0005\n",
    "x3[0]=-0.0009\n",
    "x4[0]=-0.0001\n",
    "x5[0]=0\n",
    "\n",
    "for epoch in range(nT-1):\n",
    "    x01=tf.constant(x1[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(x2[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x03=tf.constant(x3[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x04=tf.constant(x4[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x05=tf.constant(x5[epoch],shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        tape.watch(x03)\n",
    "        tape.watch(x04)\n",
    "        tape.watch(x05)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02,x03,x04,x05], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "        u1=u[:,1]\n",
    "        u2=u[:,2]\n",
    "        u3=u[:,3]\n",
    "        u4=u[:,4]\n",
    "        u5=u[:,5]\n",
    "    # Getting the derivative\n",
    "    print('V:',u0+(x01+1)**2+x02**2+x03**2+x04**2+x05**2)\n",
    "    u_x1 = tape.gradient(u0, x01)\n",
    "    u_x2 = tape.gradient(u0, x02)\n",
    "    u_x3 = tape.gradient(u0, x03)\n",
    "    u_x4 = tape.gradient(u0, x04)\n",
    "    u_x5 = tape.gradient(u0, x05)\n",
    "    u_x1 = u_x1+2*x01+2\n",
    "    u_x2 = u_x2+2*x02\n",
    "    u_x3 = u_x3+2*x03\n",
    "    u_x4 = u_x4+2*x04\n",
    "    u_x5 = u_x5+2*x05\n",
    "    l1_x1 = tape.gradient(u1, x01)\n",
    "    l2_x2 = tape.gradient(u2, x02)\n",
    "    l3_x3 = tape.gradient(u3, x03)\n",
    "    l4_x4 = tape.gradient(u4, x04)\n",
    "    l5_x5 = tape.gradient(u5, x05)\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    divL[epoch]=l1_x1.numpy()+l2_x2.numpy()+l3_x3.numpy()+l4_x4.numpy()+l5_x5.numpy()\n",
    "    \n",
    "    if ((x1[epoch]+1.0)**2+(x2[epoch])**2+(x3[epoch])**2+(x4[epoch])**2+(x5[epoch])**2)**0.5<0.01:\n",
    "        break\n",
    "    \n",
    "    p1=u_x1.numpy()\n",
    "    p2=u_x2.numpy()\n",
    "    p3=u_x3.numpy()\n",
    "    p4=u_x4.numpy()\n",
    "    p5=u_x5.numpy()\n",
    "    bx1=-2*(x1[epoch]**3-x1[epoch])-(x2[epoch]+x3[epoch]+x4[epoch]+x5[epoch])\n",
    "    bx2=-x2[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    bx3=-x3[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    bx4=-x4[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    bx5=-x5[epoch]+2*(x1[epoch]**3-x1[epoch])\n",
    "    normb=(bx1**2+bx2**2+bx3**2+bx4**2+bx5**2)**0.5\n",
    "    # normbx[epoch]=normb\n",
    "    x1[epoch+1]=x1[epoch] -h * (bx1 + p1)/normb\n",
    "    x2[epoch+1]=x2[epoch] -h * (bx2 + p2)/normb\n",
    "    x3[epoch+1]=x3[epoch] -h * (bx3 + p3)/normb\n",
    "    x4[epoch+1]=x4[epoch] -h * (bx4 + p4)/normb\n",
    "    x5[epoch+1]=x5[epoch] -h * (bx5 + p5)/normb\n",
    "\n",
    "print('epoch:',epoch)\n",
    "MPEP=tf.stack([x1[0:epoch+1],x2[0:epoch+1],x3[0:epoch+1],x4[0:epoch+1],x5[0:epoch+1]], axis=1)\n",
    "scio.savemat('MPEP.mat',{'MPEP':MPEP.numpy()})\n",
    "print(MPEP)\n",
    "integ=np.sum(divL[0:epoch+1])*h\n",
    "prefactor=1/miu2_star*(2*pi*deth2_star/detH_bar)**0.5*np.exp(integ)\n",
    "print('integ:',integ)\n",
    "print('prefactor:',prefactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approximation error\n",
    "NH=100000\n",
    "xt1=tf.random.uniform([NH],xmin,xmax,dtype=tf.float32)\n",
    "xt2=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "xt3=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "xt4=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "xt5=tf.random.uniform([NH],ymin,ymax,dtype=tf.float32)\n",
    "X_f = tf.stack([xt1,xt2,xt3,xt4,xt5], axis=1)\n",
    "\n",
    "Vtrue=(1-xt1**2)**2+xt2**2+xt3**2+xt4**2+xt5**2\n",
    "l1true=-(xt2+xt3+xt4+xt5)\n",
    "l2true=2*(xt1**3-xt1)\n",
    "l3true=2*(xt1**3-xt1)\n",
    "l4true=2*(xt1**3-xt1)\n",
    "l5true=2*(xt1**3-xt1)\n",
    "\n",
    "u = model.predict(X_f)\n",
    "u0=u[:,0]+(xt1+1)**2+xt2**2+xt3**2+xt4**2+xt5**2\n",
    "u1=u[:,1]\n",
    "u2=u[:,2]\n",
    "u3=u[:,3]\n",
    "u4=u[:,4]\n",
    "u5=u[:,5]\n",
    "\n",
    "eV=tf.reduce_max((u0-Vtrue)**2,axis=0)/tf.reduce_max((Vtrue)**2,axis=0)\n",
    "el=tf.reduce_max((u1-l1true)**2+(u2-l2true)**2+(u3-l3true)**2+(u4-l4true)**2+(u5-l5true)**2,axis=0)/tf.reduce_max((l1true)**2+(l2true)**2+(l3true)**2+(l4true)**2+(l5true)**2,axis=0)\n",
    "print(eV,el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ca9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
